{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Neural Network with the  MNIST dataset\n",
    "\n",
    "### Dependencies:\n",
    "    tensorflow\n",
    "    tf.keras \n",
    "    numpy\n",
    "    matplotlib\n",
    "    pandas\n",
    "    seaborn\n",
    "    edward\n",
    "\n",
    "### Dataset:\n",
    "    - the MNIST dataset of handwritten digits: http://yann.lecun.com/exdb/mnist/\n",
    "    - Training set: 60,000 images, Testing set: 10,000 images\n",
    "    - 10 categories: {0,1,..9}\n",
    "    - 28x28 image = 784 features \n",
    "    - nonMNIST images can be found here: http://yaroslavvb.com/upload/notMNIST/ or davidflanagan/notMNIST-to-MNIST.git \n",
    "\n",
    "    \n",
    "### Model:\n",
    "    Task: classify the handwritten MNIST digits into one of the 10 classes {0,1,2,...9} and give a measure of the uncertainty of the classificatin. \n",
    "    Model: soft-max regression \n",
    "    Likelihood function: Categorical likelihood function - to quantify the probability of the observed data given a set of parameters, weights and biases in this case. The Categorical distrubution is also known as the Multinoulli distribution. \n",
    "    Infer the posterior using Variational Inference - minimize the KL divergence between the the true posterior and approximating distributions \n",
    "    We evaluate the model with a set of predictions and their accuracies, instead of a single prediction, as per regular NNs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import seaborn as sns\n",
    "from edward.models import Categorical, Normal\n",
    "import edward as ed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import data sets - MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data using TensorFlow methos\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Edward to place priors on the weights and biases\n",
    "ed.set_seed(314159)\n",
    "N = 100 # number of images in a minibatch\n",
    "D = 784 # number of features\n",
    "K = 10 # number of classes\n",
    "\n",
    "# Create a placeholder for the data in minibatches in a TensorFlow graph\n",
    "x = tf.placeholder(tf.float32, [None, D])\n",
    "\n",
    "# place a normal Gaussian (0,1) prior on the weights and biases \n",
    "w = Normal(loc=tf.zeros([D, K]), scale=tf.ones([D,K]))\n",
    "b = Normal(loc=tf.zeros(K), scale=tf.ones(K))\n",
    "\n",
    "# Categorical likelihood for classification\n",
    "y = Categorical(tf.matmul(x,w)+b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Variational Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The posterior is not calculated directly, \n",
    "# its practically too computationally expensive \n",
    "# but we use a Variational Inference technique:\n",
    "# minimize the KL divergence between the true posterior \n",
    "# and the approximating distribution\n",
    "\n",
    "# Contruct the approximate distributions q(w) for weights and q(b) for biases\n",
    "# assume Normal distributions\n",
    "qw = Normal(loc=tf.Variable(tf.random_normal([D,K])),scale=tf.nn.softplus(tf.Variable(tf.random_normal([D,K]))))\n",
    "qb = Normal(loc=tf.Variable(tf.random_normal([K])), scale=tf.nn.softplus(tf.Variable(tf.random_normal([K]))))\n",
    "\n",
    "# Create a placeholder for the labels\n",
    "y_ph = tf.placeholder(tf.int32, [N])\n",
    "\n",
    "# Define the Variational Inference technique - \n",
    "# minimize the KL divergence \n",
    "inference = ed.KLqp({w: qw, b: qb}, data={y:y_ph})\n",
    "\n",
    "# Initliaze the inference variables\n",
    "inference.initialize(n_iter=5000, n_print=100, scale={y: float(mnist.traing.num_examples) / N})\n",
    "\n",
    "#Load a tensorflow session, initialize all the variables\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start interations - begin training \n",
    "# load the data in minibatches and update VI infernce using each new batch\n",
    "for _ in range(inference.n_iter):\n",
    "    X_batch, Y_batch = mnist.train.next_batch(N)\n",
    "    Y_batch = np.argmax(Y_batch, axis=1)\n",
    "    info_dict = inference.update(feed_dict={x: X_batch, y_ph: Y_batch})\n",
    "    #inference.print_progress(info_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with a set of predictions and accuracies\n",
    "# draw 100 samples from the posterior distribution, \n",
    "# and check performance on each sample\n",
    "\n",
    "# Load the test images and convert to a single label\n",
    "X_test = mnist.test.images\n",
    "Y_test = np.argmax(mnist.test.labels, axis=1)\n",
    "\n",
    "# Generate samples the posterior and store them\n",
    "n_samples = 100\n",
    "prob_lst = []\n",
    "samples = []\n",
    "w_samples = []\n",
    "b_samples = []\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    w_samp = qw.sample()\n",
    "    b_samp = qb.sample()\n",
    "    w_samples.append(w_samp)\n",
    "    b_samples.append(b_samp)\n",
    "    \n",
    "    #compute the probability of each class for each (w,b) sample\n",
    "    prob = tf.nn.softmax(tf.matmul(X_test, w_samp)+ b_samp)\n",
    "    prob_lst.append(prob_eval())\n",
    "    sample = tf.concat([tf.reshape(w_sampe, [-1]), b_samp],0)\n",
    "    samples.append(sample.eval())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy of the model\n",
    "accy_test=[]\n",
    "for prob in prob_lst:\n",
    "    y_trn_prd = np.argmax(prob, axis=1).astype(np.float32)\n",
    "    acc = (y_trn_prd == Y_test).mean()*100\n",
    "    acc_test.append(acc)\n",
    "    \n",
    "plt.hist(accy_test)\n",
    "plt.title(\"Histogram of prediction accuracies in the MNIST test data\")\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model averaging will give us the equivalent of a classical machine learning model\n",
    "Y_pred = np.argmax(np.mean(prob_lst,axis=0), axis=1)\n",
    "print(\"Accuracy in predicting the test data = \", (Y_pred == Y_test).mean()*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first image from the test data and its label\n",
    "test_image = X_test[0:1]\n",
    "test_label = Y_test[0]\n",
    "print(\"Truth = \", test_label)\n",
    "\n",
    "pixels = test_image.reshape((28,28))\n",
    "plt.imshow(pixels,cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what the model predicts for each (w,b) sample from the posterior\n",
    "sing_img_probs = []\n",
    "for w_samp,b_samp in zip(w_samples, b_samples):\n",
    "    prob = tf.nn.softmax(tf.matmul(X_test[0:1],w_samp)+b_samp)\n",
    "    sing_img_probs.append(prob.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of these predictions.\n",
    "plt.hist(np.argmax(sing_img_probs,axis=2),bins=range(10))\n",
    "plt.xticks(np.arange(0,10))\n",
    "plt.xlim(0,10)\n",
    "plt.xlabel(\"Accuracy of the prediction of the test digit\")\n",
    "plt.ylabel(\"Frequency\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
